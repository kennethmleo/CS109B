{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title \n",
    "Exercise: Attention\n",
    "\n",
    "## Description :\n",
    "In this exercise, you will implement an Attention mechanism. We load three encoder hidden states into `enc_states`, and 1 decoder hidden state into `dec_state`. Your task is to compute the final `context_vector`.\n",
    "\n",
    "That is, you should calculate an Attention score for every encoder hidden state, exponentiate these, then normalize them so they sum to 1. These are your Attention weights. Then, produce a context vector by multiplying each Attention weight by its corresponding encoder hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">**REMINDER**</font>: After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports useful libraries\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">YOU DO NOT NEED TO EDIT THE CELL BELOW</font>\n",
    "\n",
    "The follow code loads three encoder states into the dictionary `enc_states`, whereby the keys are 0, 1, and 2, and their respective values are lists of 50 floats (representing each hidden state). The code also populates a single list of floats, `dec_state`, which contains 50 floats (representing the hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes we're passing in several enc states but only 1 dec states\n",
    "def load_hidden_states(filename):\n",
    "    enc_states = {}\n",
    "    dec_state = []\n",
    "    \n",
    "    f = open(filename)\n",
    "    for line in f.readlines():\n",
    "        model, num = line.split()[0].split(\"_\")\n",
    "        if model == \"enc\":\n",
    "            enc_states[int(num)] = [float(t) for t in line.split(\" \")[1:]]\n",
    "        else:\n",
    "            dec_state = [float(t) for t in line.split(\" \")[1:]]\n",
    "    return enc_states, dec_state\n",
    "\n",
    "enc_states, dec_state = load_hidden_states(\"hidden_states.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">YOU DO NOT NEED TO EDIT THE CELL BELOW</font>\n",
    "\n",
    "The follow code simply computes the attention score as the dot-product between the two passed-in embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the attention score as the dot product\n",
    "def calculate_attention_score(v1, v2):\n",
    "    return sum(a*b for a, b in zip(v1, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, populate `attention_scores` with the _exponentiated_ attention scores: $e^{(\\text{score(enc_i, dec_j)})}$. The main aspect to figure out is which hidden states to pass to `calculate_attention_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_a) ###\n",
    "attention_scores = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for i in enc_states:\n",
    "    score = calculate_attention_score(enc_states[i], dec_state)\n",
    "    attention_scores.append(math.exp(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, simply _normalize_ each of the exponentiated scores and store them in `attention_weights`. They should sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_b) ###\n",
    "attention_weights = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "total = sum(attention_scores)\n",
    "\n",
    "for score in attention_scores:\n",
    "    attention_weights.append(score / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, create the final context vector `context_vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_c) ###\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "context_vector = [0.0] * len(dec_state)  # initialize with zeros (same dimension as hidden states)\n",
    "\n",
    "for i, weight in enumerate(attention_weights):\n",
    "    enc_vec = enc_states[i]\n",
    "    for j in range(len(enc_vec)):\n",
    "        context_vector[j] += weight * enc_vec[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13577677979834138,\n",
       " 0.36422322020165854,\n",
       " 0.19833628614204235,\n",
       " 0.5016637138579575]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
