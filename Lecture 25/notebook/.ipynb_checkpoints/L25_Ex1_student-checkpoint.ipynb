{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title \n",
    "Exercise: Self-Attention\n",
    "\n",
    "## Description :\n",
    "In this exercise, you will implement a Self-Attention Head for the **3rd word** of a 4-word input. From a Pickled file, we load `query`, `key`, and `value` vectors that corresponds to 4 different inputs (thus, a total of 12 vectors). Specifically, this is loaded into 3 respective `dicts`.\n",
    "\n",
    "You only need to calculate the final `z` \"context\" vector that corresponds to the 3rd word (i.e., `z2`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">**REMINDER**</font>: After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports useful libraries\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">YOU DO NOT NEED TO EDIT THE CELL BELOW</font>\n",
    "\n",
    "The follow code loads the `queries`, `keys`, and `values` vectors that correspond to 4 distinct words. Here, all vectors have a length of 25. Each of these variables (e.g., `queries`) is a `dict`, indexed by the word number. For example `queries[2]` corresponds to the 3rd word (it's 0-indexed), and its value is a list of length 25, which corresponds to the actual _query_ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L25.p file has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create random queries, keys, values for 4 words, each of length 25\n",
    "queries = {i: np.random.randn(25).tolist() for i in range(4)}\n",
    "keys = {i: np.random.randn(25).tolist() for i in range(4)}\n",
    "values = {i: np.random.randn(25).tolist() for i in range(4)}\n",
    "\n",
    "# Save to L25.p\n",
    "with open(\"L25.p\", \"wb\") as f:\n",
    "    pickle.dump([queries, keys, values], f)\n",
    "\n",
    "print(\"L25.p file has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query vector for 3rd word: [0.31255341745661447, 0.5444790208080438, 0.8632430499129878, -1.051515695088019, 0.724062761084608, 1.7988668525358067, -2.335102779369179, -0.7719423128312118, -0.9059941854398721, 0.8248369346074708, 0.631856726562657, -0.5425496355876188, 0.2436146146419746, 0.5162508478350442, 1.2239919241719166, -1.0909014329506188, -0.3726517089845168, 2.239444930344251, 1.157228796616806, -0.9081594626858468, -0.5572001524430997, -1.7401066957543887, 0.387705323294313, -0.3395043240436249, -1.268354233503594]\n",
      "\n",
      "key vector for 3rd word: [-0.9008351905899606, 0.38919556285089085, 0.7124181581634611, 1.3423880876814092, -0.7406983167245698, 0.6855559198309097, 0.4734445175347144, -0.240820312768575, 0.44226398751729934, -1.6197564972256955, 0.11427482289977442, -0.9939044298094357, 0.051007671999343945, 0.34562879601223684, 1.4798319833963203, -1.7398377320192, -0.8878764079461707, -2.143627243195943, -0.9348044579989998, -0.35917632890486406, -0.08221775624778214, -1.7322943195140703, 0.11930063572677652, -1.11856145440013, -0.9462410960672338]\n",
      "\n",
      "value vector for 3rd word: [0.7560901959870673, 2.6321499797705967, 0.5961319630382214, -0.9572646185947492, 0.1687539300602293, -0.457062414898862, -0.6125398350879356, 0.032157454056700986, -0.5274673470124892, 0.8016517343321352, 1.329544892558626, 0.3946840441906536, -0.16315792837066578, 1.0164106646720263, 0.19857569638148556, 0.2547986676262081, -0.14408435216569315, 0.43681338879689385, 1.143481442736725, 2.0864373877792315, -1.1182942082057175, 0.3328016558153074, -0.43056544483137693, 0.671793852954609, 0.5737053634838828]\n"
     ]
    }
   ],
   "source": [
    "pickled_content = pickle.load(open(\"L25.p\", \"rb\"))\n",
    "queries, keys, values = [pickled_content[i] for i in range(3)]\n",
    "\n",
    "# to illustrate, let's print the query, key, and value vectors that correspond to teh 3rd word in the sentence:\n",
    "print(\"query vector for 3rd word:\", queries[2])\n",
    "print(\"\\nkey vector for 3rd word:\", keys[2])\n",
    "print(\"\\nvalue vector for 3rd word:\", values[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">YOU DO NOT NEED TO EDIT THE CELL BELOW</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the dot product of two passed-in vectors\n",
    "def calculate_dot_product(v1, v2):\n",
    "    return sum(a*b for a, b in zip(v1, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, populate the `self_attention_scores` list by calculating the **four** Attention scores that correspond to the **3rd word**. Each Attention score should be divided by $\\sqrt{d_k}$ (where $d_k$ represents the length of the key vector), and the ordering should be natural. That is, the 1st item in `self_attention_scores` should correspond to the Attention score for the 1st word, the 2nd item in `self_attention_scores` should correspond to the Attention score for 2nd word, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3163311366417093,\n",
       " -2.006985897795121,\n",
       " 0.22941007018224074,\n",
       " -0.8547962628439226]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### edTest(test_a) ###\n",
    "self_attention_scores = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "scaling_factor = math.sqrt(len(keys[0]))\n",
    "\n",
    "# Query for the 3rd word\n",
    "query_vector = queries[2]\n",
    "\n",
    "# Loop through all 4 words and compute attention scores\n",
    "for i in range(len(keys)):\n",
    "    score = calculate_dot_product(query_vector, keys[i]) / scaling_factor\n",
    "    self_attention_scores.append(score)\n",
    "\n",
    "self_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, populate the `softmax` list by calculating the softmax of each of the four Attention scores found in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4301602867829034,\n",
       " 0.04213340376804944,\n",
       " 0.3943492084956433,\n",
       " 0.13335710095340383]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### edTest(test_b) ###\n",
    "softmax_scores = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# exponentiate each score\n",
    "exp_scores = [math.exp(score) for score in self_attention_scores]\n",
    "\n",
    "# sum of exponentiated scores\n",
    "total = sum(exp_scores)\n",
    "\n",
    "# normalize each score\n",
    "softmax_scores = [exp_score / total for exp_score in exp_scores]\n",
    "\n",
    "softmax_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, create the final $z2$ list that corresponds to the 3rd word. $z2$ should have a length of 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4988362061113303,\n",
       " 1.0399374204971337,\n",
       " 0.010786855250006722,\n",
       " -0.505420903537367,\n",
       " 0.16399025809114082,\n",
       " 0.0951801722193798,\n",
       " -0.6380246648103174,\n",
       " -0.372752786449124,\n",
       " -0.45656799324064706,\n",
       " 0.1762284678524646,\n",
       " 0.3443050534150157,\n",
       " -0.071734330410285,\n",
       " -0.5963679511335237,\n",
       " 0.1144371536018865,\n",
       " -0.4253568999284065,\n",
       " -0.162668014617458,\n",
       " 0.400086978244591,\n",
       " 0.9843709691966013,\n",
       " 0.43310929900201445,\n",
       " 0.35917217039833244,\n",
       " 0.1591683180161405,\n",
       " 0.7356985907661435,\n",
       " 0.12465708738832049,\n",
       " 0.9934073233690177,\n",
       " -0.7258839098784635]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### edTest(test_c) ###\n",
    "z2 = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for dim in range(len(values[0])):  \n",
    "    weighted_sum = 0\n",
    "    # Sum over all 4 words\n",
    "    for i in range(len(values)):  \n",
    "        weighted_sum += softmax_scores[i] * values[i][dim]\n",
    "    z2.append(weighted_sum)\n",
    "\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
