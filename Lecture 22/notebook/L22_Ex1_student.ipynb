{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title :\n",
    "Unigram  LM\n",
    "\n",
    "## Description :\n",
    "Text data is unlike the typical \"design matrix\", i.i.d. data that we've often worked with. Here, you'll gain practice working with actual words, as you'll parse, count, and calculate a probability.\n",
    "\n",
    "An individual unigram's likelihood (**unsmoothed**) is defined as:\n",
    "\n",
    "$$L\\left(w\\right)=\\frac{n_w\\left(D_t\\right)}{n_o\\left(D_t\\right)}$$\n",
    "\n",
    "where the numerator represents the number of times word $w$ appeared in the training corpus $D_t$.\n",
    "\n",
    "For this exercise, we will define the **smoothed** unigram's likelihood as:\n",
    "\n",
    "$$L\\left(w\\right)=\\frac{n_w\\left(D_t\\right)\\ +\\alpha}{n_o\\left(D_t\\right)\\ +\\alpha\\left|V\\right|}$$\n",
    "\n",
    "where $\\alpha$ is a specified real-valued number (doesn't have to be an integer), and $|V|$ is the cardinality of the lexicon (i.e., the number of distinct word types in the vocabulary)\n",
    "\n",
    "The likelihood of a new sequence $H$ is simply defined by the likelihood of each token, multiplied by each other:\n",
    "\n",
    "$$L\\left(H\\right)=\\prod_{w\\ \\in H}^{ }L\\left(w\\right)$$\n",
    "\n",
    "## HINTS :\n",
    "Depending on your approach, these functions could help you:\n",
    "\n",
    "- `re.sub()` (regular expression)\n",
    "- `.split()`\n",
    "- `.lower()`\n",
    "- `.strip()`\n",
    "- `.replace()`\n",
    "- `.sum()`\n",
    "- `defaultdict` data structure\n",
    "- `Counter` data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">**REMINDER**</font>: After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports some libraries you might find useful\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary for our experiments\n",
    "training_file = \"ex1_train.txt\"\n",
    "dev_file = \"ex1_dev.txt\"\n",
    "punctuation = ['.', '!', '?']\n",
    "\n",
    "sample1 = \"I love data science!\"\n",
    "sample2 = \"I love NLP!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `parse_string()` which takes as input a string (e.g., the contents of a file). It should return this text as a list of tokens. Specifically, the tokens should:\n",
    "- be lowercased\n",
    "- be separated by whitespace and any character present in the list of `punctuation`.\n",
    "- include no trailing or preceeding whitespace (none of the returned tokens should be of white space or empty)\n",
    "\n",
    "For example, if the input is **\" I   LOVE daTa!!\"**, it should return **[\"i\", love\", \"data\", \"!\", \"!\"]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_a) ###\n",
    "def parse_string(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Add space around each punctuation so they're treated as tokens\n",
    "    for p in punctuation:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    # Split by whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "# DO NOT EDIT THE LINES BELOW\n",
    "text = open(training_file).read()\n",
    "tokens = parse_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'data', 'science', '.', 'i', 'love', 'computer', 'science', '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `count_tokens()` that takes a list of tokens and simply outputs a dictionary-style count of the items. For example, if the input is **['run', 'forrest', 'run']**, it should return a `dict`, `defaultdict`, or `Counter` with 2 keys: **{'run':2, 'forrest':1}** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_b) ###\n",
    "def count_tokens(tokens): \n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    word_counts = Counter(tokens)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return word_counts\n",
    "\n",
    "# DO NOT EDIT THIS LINE\n",
    "word_counts = count_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'i': 2,\n",
       "         'love': 2,\n",
       "         'science': 2,\n",
       "         'data': 1,\n",
       "         '.': 1,\n",
       "         'computer': 1,\n",
       "         '!': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `calculate_likelihood()` that takes `tokens` (a list of strings) and `word_counts` (dictionary-type) and returns the likelihood of the sequence of tokens. You will run your function with the tokens parsed from the `sample1` string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_c) ###\n",
    "def calculate_likelihood(tokens, word_counts):\n",
    "    total_likelihood = 1\n",
    "\n",
    "    # Total number of tokens in the training corpus\n",
    "    total_count = sum(word_counts.values())\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in word_counts:\n",
    "            total_likelihood *= word_counts[token] / total_count\n",
    "        else:\n",
    "            total_likelihood = 0\n",
    "            break  # No need to continue if any probability is zero\n",
    "    \n",
    "    return total_likelihood\n",
    "\n",
    "# DO NOT EDIT THE LINES BELOW\n",
    "sample1_tokens = parse_string(sample1)\n",
    "likelihood = calculate_likelihood(sample1_tokens, word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.000000000000003e-05"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `calculate_smoothed_likelihood()` that is the same as the previous function but includes a smoothing parameter `alpha`. Again, you should return the likelihood of the sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_d) ###\n",
    "\n",
    "def calculate_smoothed_likelihood(alpha, tokens, word_counts):\n",
    "\n",
    "    total_likelihood = 1\n",
    "    \n",
    "    # Total tokens in training corpus\n",
    "    total_count = sum(word_counts.values())\n",
    "    # Vocabulary size\n",
    "    vocab_size = len(word_counts)\n",
    "\n",
    "    for token in tokens:\n",
    "        # Apply smoothed probability\n",
    "        token_count = word_counts.get(token, 0)\n",
    "        prob = (token_count + alpha) / (total_count + alpha * vocab_size)\n",
    "        total_likelihood *= prob\n",
    "    \n",
    "    return total_likelihood\n",
    "\n",
    "\n",
    "# DO NOT EDIT THE LINES BELOW\n",
    "sample1_tokens = parse_string(sample1)\n",
    "sample1_likelihood = calculate_smoothed_likelihood(0.5, sample1_tokens, word_counts)\n",
    "\n",
    "sample2_tokens = parse_string(sample2)\n",
    "sample2_likelihood = calculate_smoothed_likelihood(0.5, sample2_tokens, word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.840318429828834e-05, 0.00014112573173691902)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1_likelihood, sample2_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
